{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e36de5",
   "metadata": {},
   "source": [
    "# Visualización de conceptos\n",
    "\n",
    "Este Notebook no requiere ejecución de código. Su objetivo es poner en práctica los conceptos que se han visto en clase utilizando la herramienta [https://playground.tensorflow.org/](https://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ef21e",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Clasificación Lineal Simple\n",
    "\n",
    "**Objetivo:** Comprender cómo una red neuronal simple con una capa y una neurona puede resolver problemas linealmente separables.\n",
    "\n",
    "Este ejercicio te permitirá observar el comportamiento de la red neuronal más simple posible en un problema que puede ser resuelto mediante una separación lineal. Es importante entender que algunos problemas no requieren arquitecturas complejas y pueden ser resueltos eficientemente con modelos simples.\n",
    "\n",
    "### Configuración:\n",
    "- **Conjunto de Datos:**: Clasificación. Gaussian. (Separables por corte vertical; seleccionar de las opciones disponibles)\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 1\n",
    "- **Número de Neuronas:** 1 neurona\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** Lineal\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Parámetros del Dataset:\n",
    "- **Train:** 90% (porcentaje de datos para entrenamiento)\n",
    "- **Noise:** 50% (nivel de ruido en los datos)\n",
    "- **Batch:** 10 (tamaño del lote para el entrenamiento)\n",
    "- **Marcar:** \"Show test data\"\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Por qué una sola neurona es suficiente para este problema?\n",
    "- ¿Cómo afectan los parámetros del dataset al entrenamiento?\n",
    "- ¿Qué representa la frontera de decisión generada?\n",
    "- ¿Por qué la activación lineal funciona en este caso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be26f16",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Modificando el Número de Neuronas\n",
    "\n",
    "**Objetivo:** Observar el efecto de añadir neuronas en un problema que ya es linealmente separable.\n",
    "\n",
    "En este ejercicio exploraremos qué sucede cuando aumentamos la complejidad de nuestra red neuronal para un problema que ya puede ser resuelto de forma simple. Esto es importante para entender cuándo más complejidad no necesariamente significa mejor rendimiento.\n",
    "\n",
    "### Configuración Primera Variante:\n",
    "- **Conjunto de Datos:** \"Corte vertical\" (mantener del ejercicio anterior)\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 1\n",
    "- **Número de Neuronas:** 3 neuronas\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** Lineal\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Configuración Segunda Variante:\n",
    "- **Conjunto de Datos:** \"Corte vertical\" (mantener del ejercicio anterior)\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 2\n",
    "- **Número de Neuronas:** 2 neuronas por capa\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** Lineal\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Cambia significativamente la frontera de decisión al añadir más neuronas?\n",
    "- ¿Se mantiene la naturaleza lineal de la separación?\n",
    "- ¿Hay diferencias en la velocidad de convergencia?\n",
    "- ¿Qué ventajas o desventajas observas con mayor complejidad?\n",
    "- ¿Por qué la red sigue aprendiendo una frontera lineal similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7698a21",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Overfitting - Varianza\n",
    "\n",
    "**Objetivo:** Explorar el concepto de overfitting creando una red excesivamente compleja para un problema simple.\n",
    "\n",
    "Este ejercicio te permitirá observar qué sucede cuando utilizamos una arquitectura muy compleja para resolver un problema simple. Es fundamental entender los conceptos de sesgo (bias) y varianza en el aprendizaje automático.\n",
    "\n",
    "### Configuración Primera Variante (Complejidad Máxima + Activación Lineal):\n",
    "- **Conjunto de Datos:** \"Corte vertical\"\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** Máximo disponible\n",
    "- **Número de Neuronas:** Máximo disponible por capa\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** Lineal\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Configuración Segunda Variante (Complejidad Máxima + ReLU):\n",
    "- **Conjunto de Datos:** \"Corte vertical\"\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** Máximo disponible\n",
    "- **Número de Neuronas:** Máximo disponible por capa\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** **ReLU** <- ¡Cambia!\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Qué diferencias observas entre la activación Lineal y ReLU en una red muy compleja?\n",
    "- ¿Se produce overfitting? ¿Cómo lo identificas?\n",
    "- ¿Por qué una red muy compleja con activación lineal funciona bien pero con ReLU no?\n",
    "- ¿Qué papel juega la función de activación en el overfitting?\n",
    "- ¿Cómo se comporta la pérdida en entrenamiento vs validación?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5f248",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Regularización para Controlar Overfitting\n",
    "\n",
    "**Objetivo:** Aprender cómo la regularización L2 puede ayudar a controlar el overfitting en redes complejas.\n",
    "\n",
    "Continuando con el ejercicio anterior, ahora exploraremos cómo las técnicas de regularización pueden mitigar el problema del overfitting sin necesidad de reducir la complejidad arquitectural de la red.\n",
    "\n",
    "### Configuración:\n",
    "- **Conjunto de Datos:** \"Corte vertical\"\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 5\n",
    "- **Número de Neuronas:** 5\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** ReLU\n",
    "- **Regularización:** L2 (experimenta con diferentes valores)\n",
    "\n",
    "### Regularización L2\n",
    "\n",
    "La L2 Regularization (también conocida como weight decay) es una técnica para evitar el sobreajuste en modelos de machine learning.\n",
    "Lo que hace es penalizar pesos grandes en el modelo. La intuición es:\n",
    "\n",
    "- Si los pesos son muy grandes, el modelo se ajusta demasiado a los datos de entrenamiento.\n",
    "- Si los pesos son pequeños y distribuidos, el modelo tiende a ser más simple y generalizar mejor.\n",
    "\n",
    "La **regularización L2** añade un término que penaliza los pesos grandes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{total}(\\mathbf{w}) = \\mathcal{L}_{data}(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_2^2\n",
    "$$\n",
    "\n",
    "con\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{w}\\|_2^2 = \\sum_{j=1}^d w_j^2,\n",
    "$$\n",
    "\n",
    "donde $\\lambda > 0$ controla la intensidad de la penalización.\n",
    "\n",
    "La regla de actualización de un peso $w_j$ con regularización L2 es:\n",
    "\n",
    "$$\n",
    "w_j \\;\\leftarrow\\; w_j - \\eta \\left( \\frac{\\partial \\mathcal{L}_{\\text{data}}}{\\partial w_j} \\;+\\; 2\\lambda w_j \\right)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\eta$ es la tasa de aprendizaje (learning rate),\n",
    "- $\\lambda$ controla la fuerza de la regularización.\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Cómo afecta la regularización L2 al overfitting observado en el ejercicio anterior?\n",
    "- ¿Qué sucede con diferentes valores de regularización (muy bajo, medio, muy alto)?\n",
    "- ¿Se mantiene la capacidad de la red para resolver el problema?\n",
    "- ¿Cómo cambia la frontera de decisión con regularización?\n",
    "- ¿Cuál es el trade-off entre sesgo y varianza al aplicar regularización?\n",
    "- ¿Por qué la regularización es efectiva para controlar redes muy complejas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a038c",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Problemas No Lineales - Dataset XOR\n",
    "\n",
    "**Objetivo:** Comprender las limitaciones de las redes lineales y la necesidad de funciones de activación no lineales.\n",
    "\n",
    "El problema XOR es un caso clásico que no puede ser resuelto por un perceptrón simple debido a que no es linealmente separable. Este ejercicio demuestra la importancia de las funciones de activación no lineales y la profundidad en las redes neuronales.\n",
    "\n",
    "### Parámetros del Dataset:\n",
    "- **Train:** 90% (porcentaje de datos para entrenamiento)\n",
    "- **Noise:** 25% (nivel de ruido en los datos) <- ¡Cambio!\n",
    "- **Batch:** 10 (tamaño del lote para el entrenamiento)\n",
    "- **Marcar:** \"Show test data\"\n",
    "\n",
    "**Reducimos el nivel de ruido**\n",
    "\n",
    "### Configuración Variante 1 (Sin Activación):\n",
    "- **Conjunto de Datos:** XOR\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 0 (solo capa de salida)\n",
    "- **Número de Neuronas:** 1\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** Ninguna (lineal)\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Configuración Variante 2 (Una Capa):\n",
    "- **Conjunto de Datos:** XOR\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 1\n",
    "- **Número de Neuronas:** Variable (experimentar)\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** Ninguna (lineal)\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Configuración Variante 3 (Dos Capas):\n",
    "- **Conjunto de Datos:** XOR\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 2\n",
    "- **Número de Neuronas:** 4 neuronas en primera capa, 2 en segunda\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** Ninguna (lineal)\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Por qué una red sin funciones de activación no lineales no puede resolver el problema XOR? (independientemente del número de capas o neuronas)\n",
    "- ¿Qué patrón sigue el problema XOR y por qué no es linealmente separable?\n",
    "- ¿Cómo se ve la frontera de decisión de cada configuración lineal (0, 1 o 2 capas) al intentar aprender XOR?\n",
    "- ¿Qué efecto tendría agregar funciones de activación no lineales en una red para resolver XOR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e127011",
   "metadata": {},
   "source": [
    "## Ejercicio 6: Arquitecturas No Lineales y Neuronas Muertas\n",
    "\n",
    "**Objetivo:** Explorar el funcionamiento de las funciones de activación ReLU y el fenómeno de las neuronas muertas.\n",
    "\n",
    "Ahora que hemos visto las limitaciones de las redes lineales, exploraremos cómo las funciones de activación no lineales permiten resolver problemas complejos, pero también introducen nuevos desafíos como las neuronas muertas.\n",
    "\n",
    "### Configuración Primera Variante (4x4 con ReLU):\n",
    "- **Conjunto de Datos:** XOR\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 2\n",
    "- **Número de Neuronas:** 4 neuronas por capa\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** ReLU\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Configuración Segunda Variante (4x2 con ReLU):\n",
    "- **Conjunto de Datos:** XOR\n",
    "- **Características de Entrada:** X₁ y X₂ (por defecto)\n",
    "- **Número de Capas:** 2\n",
    "- **Número de Neuronas:** 4 neuronas en primera capa, 2 en segunda\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** ReLU\n",
    "- **Regularización:** Ninguna\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Logra la red resolver el problema XOR con activación ReLU?\n",
    "- ¿Observas neuronas que se \"mueren\" (permanecen inactivas)?\n",
    "- ¿Por qué algunas configuraciones fallan en converger?\n",
    "- ¿Cómo afecta el número de neuronas a la estabilidad del entrenamiento?\n",
    "- ¿Qué significa \"encontrar el punto dulce en la arquitectura\"?\n",
    "- ¿Cuál es la relación entre complejidad arquitectural y capacidad de aprendizaje?\n",
    "- ¿Por qué la inicialización de pesos puede afectar el resultado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9dc6d0",
   "metadata": {},
   "source": [
    "## Ejercicio 7: Exploración del Dataset Círculo\n",
    "\n",
    "**Objetivo:** Analizar cómo las redes neuronales manejan patrones con simetría radial.\n",
    "\n",
    "El dataset de círculo presenta un tipo diferente de separación no lineal donde los datos están organizados en un patrón circular. Este ejercicio te permitirá explorar cómo las redes neuronales pueden aprender fronteras de decisión curvas y complejas.\n",
    "\n",
    "### Configuración Inicial:\n",
    "- **Conjunto de Datos:** Círculo\n",
    "- **Características de Entrada:** Solo X₁ y X₂ (sin modificaciones adicionales)\n",
    "- **Número de Capas:** Experimenta con diferentes valores\n",
    "- **Número de Neuronas:** Experimenta con diferentes configuraciones\n",
    "- **Tasa de Aprendizaje:** 0.03 (ajusta según sea necesario)\n",
    "- **Activación:** Experimenta con diferentes funciones\n",
    "- **Regularización:** Experimenta según el comportamiento observado\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Qué tipo de frontera de decisión se necesita para separar este dataset?\n",
    "- ¿Cuál es la arquitectura mínima necesaria para resolver este problema?\n",
    "- ¿Cómo se compara la dificultad de este problema con XOR?\n",
    "- ¿Qué funciones de activación funcionan mejor para este tipo de geometría?\n",
    "- ¿Necesitas más o menos neuronas comparado con XOR? ¿Por qué?\n",
    "- ¿Cómo afecta la simetría del problema al aprendizaje?\n",
    "- ¿Observas algún patrón en cómo la red construye la frontera circular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94f73c",
   "metadata": {},
   "source": [
    "## Ejercicio 8: Exploración del Dataset Espiral\n",
    "\n",
    "**Objetivo:** Enfrentar uno de los problemas de clasificación más desafiantes con patrones espirales entrelazados.\n",
    "\n",
    "El dataset espiral representa uno de los problemas de clasificación más complejos disponibles en TensorFlow Playground. Los datos forman espirales entrelazadas que requieren fronteras de decisión muy sofisticadas y redes con alta capacidad expresiva.\n",
    "\n",
    "### Configuración Inicial:\n",
    "- **Conjunto de Datos:** Espiral\n",
    "- **Características de Entrada:** Solo X₁ y X₂ (sin modificaciones adicionales)\n",
    "- **Número de Capas:** Experimenta con diferentes profundidades\n",
    "- **Número de Neuronas:** Experimenta con diferentes anchuras\n",
    "- **Tasa de Aprendizaje:** 0.03 (puede requerir ajustes)\n",
    "- **Activación:** Experimenta con diferentes funciones\n",
    "- **Regularización:** Monitorea el overfitting\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Cuál es la arquitectura mínima capaz de separar las espirales?\n",
    "- ¿Cómo se compara la complejidad requerida con los datasets anteriores?\n",
    "- ¿Qué forma tiene la frontera de decisión final?\n",
    "- ¿Es necesario usar regularización para evitar overfitting?\n",
    "- ¿Cómo afecta la tasa de aprendizaje a la convergencia?\n",
    "- ¿Observas diferencias significativas entre funciones de activación?\n",
    "- ¿Qué estrategias de arquitectura (más profunda vs más ancha) funcionan mejor?\n",
    "- ¿Por qué este problema es considerado uno de los más desafiantes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a007921",
   "metadata": {},
   "source": [
    "## Ejercicio 9: Experimento con Dropout y Manipulación de Pesos\n",
    "\n",
    "**Objetivo:** Observar la robustez y adaptabilidad de las redes neuronales mediante la manipulación manual de pesos.\n",
    "\n",
    "Este ejercicio final te permitirá experimentar con la manipulación directa de los pesos de la red para entender cómo se adapta el sistema cuando algunas conexiones se eliminan o se modifican. Es una forma práctica de visualizar conceptos como dropout y la redundancia en las redes neuronales.\n",
    "\n",
    "### Configuración Base:\n",
    "- **Conjunto de Datos:** Cualquiera de los datasets anteriores (recomendado: XOR o Círculo)\n",
    "- **Características de Entrada:** X₁ y X₂\n",
    "- **Número de Capas:** 2-3 capas\n",
    "- **Número de Neuronas:** Configuración estable que funcione bien\n",
    "- **Tasa de Aprendizaje:** 0.03\n",
    "- **Activación:** ReLU o Tanh\n",
    "- **Regularización:** Ninguna inicialmente\n",
    "\n",
    "### Procedimiento del Experimento:\n",
    "1. **Entrenar la red** hasta que funcione correctamente\n",
    "2. **Pausar el entrenamiento**\n",
    "3. **Hacer clic en un peso** para ponerlo a cero (simular neurona muerta)\n",
    "4. **Reanudar el entrenamiento** y observar la adaptación\n",
    "5. **Repetir** con diferentes pesos y combinaciones\n",
    "\n",
    "### Preguntas a resolver:\n",
    "- ¿Cómo se adapta la red cuando eliminas un peso importante?\n",
    "- ¿Qué sucede si eliminas múltiples pesos simultáneamente?\n",
    "- ¿Hay pesos más críticos que otros para el funcionamiento de la red?\n",
    "- ¿La red puede recuperarse y mantener su rendimiento?\n",
    "- ¿Cómo se redistribuyen las responsabilidades entre las neuronas restantes?\n",
    "- ¿Qué nos enseña esto sobre la redundancia en las redes neuronales?\n",
    "- ¿Cómo se relaciona este experimento con técnicas como dropout?\n",
    "- ¿Observas diferencias según el momento del entrenamiento en que elimines los pesos?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
