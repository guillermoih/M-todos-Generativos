{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajlSB55Rro99"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5100,
     "status": "ok",
     "timestamp": 1730814194142,
     "user": {
      "displayName": "Punto",
      "userId": "10186899819214684786"
     },
     "user_tz": -60
    },
    "id": "zUT_R0lqrnVN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, ZeroPadding2D, Flatten, Dense, MaxPooling2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhBCtMLtvuTb"
   },
   "source": [
    "## 1. Comparación: Capas Convolucionales vs Capas Densas\n",
    "\n",
    "Una de las principales ventajas de las redes convolucionales es su **eficiencia en parámetros**. Vamos a comparar cuántos parámetros necesita una capa convolucional frente a una capa densa para procesar la misma entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1730814871092,
     "user": {
      "displayName": "Punto",
      "userId": "10186899819214684786"
     },
     "user_tz": -60
    },
    "id": "EVzuoVpkvuD2",
    "outputId": "77f4ec9b-e2f9-413b-bd23-7a71ec390483"
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(27, 27, 1))\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1,1))(input)\n",
    "\n",
    "print(f\"Entrada: {input.shape}\")\n",
    "print(f\"Salida: {x.shape}\")\n",
    "print(f\"Filtros: 64, Kernel: 3x3, Padding: same, Stride: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730814871093,
     "user": {
      "displayName": "Punto",
      "userId": "10186899819214684786"
     },
     "user_tz": -60
    },
    "id": "C93ArZHAM6bv",
    "outputId": "cabe36a9-f99b-48c0-ef87-d6605e50be8a"
   },
   "outputs": [],
   "source": [
    "model_conv = Model(input, x)\n",
    "model_conv.summary()\n",
    "print(f\"\\nPesos del filtro: 3 × 3 × 1 × 64 = {3*3*1*64:,} parámetros\")\n",
    "print(f\"Bias: 64 parámetros\") \n",
    "print(f\"Total: {3*3*1*64 + 64:,} parámetros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1730814310038,
     "user": {
      "displayName": "Punto",
      "userId": "10186899819214684786"
     },
     "user_tz": -60
    },
    "id": "Bc-QDGpKkMlf",
    "outputId": "19c2d18d-04bb-46d3-d2a7-b24ae4748c2b"
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(27, 27, 1))\n",
    "x = Flatten()(input)  # Convierte (27, 27, 1) → (729,)\n",
    "x = Dense(64)(x)      # 64 neuronas de salida\n",
    "\n",
    "print(f\"Entrada original: (27, 27, 1)\")\n",
    "print(f\"Después de Flatten: {x.shape[1]} elementos\")\n",
    "print(f\"Salida Dense: {x.shape}\")\n",
    "print(f\"Neuronas: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730814310792,
     "user": {
      "displayName": "Punto",
      "userId": "10186899819214684786"
     },
     "user_tz": -60
    },
    "id": "sNpKhjjAkjT_",
    "outputId": "a5b272ea-6d4f-471f-9938-43bd44dd150c"
   },
   "outputs": [],
   "source": [
    "model_dense = Model(input, x)\n",
    "print(\"PARÁMETROS CAPA DENSA:\")\n",
    "model_dense.summary()\n",
    "print(f\"\\nPesos: 729 × 64 = {729*64:,} parámetros\")\n",
    "print(f\"Bias: 64 parámetros\")\n",
    "print(f\"Total: {729*64 + 64:,} parámetros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Capa Convolucional: {3*3*1*64 + 64:,} parámetros\")\n",
    "print(f\"Capa Densa: {729*64 + 64:,} parámetros\")\n",
    "print(f\"Ratio: {((729*64 + 64) / (3*3*1*64 + 64)):.1f}x más parámetros en la capa densa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Efectos del Padding\n",
    "\n",
    "Vamos a ver cómo el tipo de padding afecta las dimensiones de salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PADDING = 'SAME' (mantiene dimensiones)\n",
    "input = Input(shape=(28, 28, 1))\n",
    "\n",
    "# Diferentes tamaños de kernel con padding='same'\n",
    "kernels = [3, 5, 7]\n",
    "for k in kernels:\n",
    "    x = Conv2D(filters=32, kernel_size=(k, k), padding='same', strides=1)(input)\n",
    "    print(f\"Kernel {k}x{k}: {input.shape} → {x.shape}\")\n",
    "    \n",
    "print(f\"\\n Con padding='same' y stride=1, las dimensiones se mantienen iguales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PADDING = 'VALID' (reduce dimensiones)\n",
    "input = Input(shape=(28, 28, 1))\n",
    "\n",
    "for k in kernels:\n",
    "    x = Conv2D(filters=32, kernel_size=(k, k), padding='valid', strides=1)(input)\n",
    "    reduction = 28 - x.shape[1]\n",
    "    print(f\"Kernel {k}x{k}: {input.shape} → {x.shape} (reducción: {reduction} píxeles)\")\n",
    "\n",
    "print(f\"\\n Con padding='valid', se pierden píxeles en los bordes\")\n",
    "print(f\"Fórmula: output_size = input_size - kernel_size + 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Efectos del Stride (Paso de la Convolución)\n",
    "\n",
    "El stride controla cuánto **downsampling** (reducción de dimensiones) se produce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARACIÓN DE DIFERENTES STRIDES\n",
    "input = Input(shape=(32, 32, 3))  # Imagen RGB típica\n",
    "\n",
    "strides = [1, 2, 3, 4]\n",
    "for s in strides:\n",
    "    x = Conv2D(filters=64, kernel_size=3, padding='same', strides=s)(input)\n",
    "    reduction_factor = input.shape[1] / x.shape[1]\n",
    "    print(f\"Stride {s}: {input.shape} → {x.shape} (factor reducción: {reduction_factor:.1f}x)\")\n",
    "\n",
    "print(f\"\\n Regla: Con padding='same', output_size = input_size / stride\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIDE CON PADDING='VALID'\n",
    "input = Input(shape=(28, 28, 1))\n",
    "\n",
    "for s in [1, 2, 3]:\n",
    "    x = Conv2D(filters=32, kernel_size=5, padding='valid', strides=s)(input)\n",
    "    # Fórmula: (input_size - kernel_size + 1) / stride\n",
    "    expected = (28 - 5 + 1) // s\n",
    "    print(f\"Stride {s}: {input.shape} → {x.shape} (esperado: {expected})\")\n",
    "\n",
    "print(f\"\\n Stride alto con padding='valid' puede reducir drásticamente las dimensiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Múltiples Convoluciones Anidadas\n",
    "\n",
    "En redes reales, las convoluciones se **apilan** para crear jerarquías de características. Veamos cómo evolucionan las dimensiones a través de múltiples capas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RED CONVOLUCIONAL TÍPICA (como LeNet/VGG)\n",
    "\n",
    "input = Input(shape=(224, 224, 3))  # ImageNet input\n",
    "x = input\n",
    "\n",
    "# Bloque 1\n",
    "x = Conv2D(8, (3, 3), padding='same', name='conv1_1')(x)\n",
    "print(f\"Conv1_1: {x.shape}\")\n",
    "x = Conv2D(16, (3, 3), padding='same', name='conv1_2')(x) \n",
    "print(f\"Conv1_2: {x.shape}\")\n",
    "print()\n",
    "\n",
    "# Embudo\n",
    "print(\"También MaxPooling2D para reducir dimensiones, pero aquí usamos Conv2D con stride 2\")\n",
    "x = Conv2D(32, (3, 3), padding='same', name='pool1', strides=(3, 3))(x)\n",
    "print(f\"Reducción1: {x.shape}\")\n",
    "x = Conv2D(64, (3, 3), padding='same', name='pool2', strides=(3, 3))(x)\n",
    "print(f\"Reducción2: {x.shape}\")\n",
    "print()\n",
    "\n",
    "# Densa\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation='relu', name='fc')(x)\n",
    "print(f\"Densa: {x.shape}\")\n",
    "x = Dense(3, activation='softmax', name='output')(x) # 3 clases\n",
    "print(f\"Salida: {x.shape}\")\n",
    "model = Model(input, x)\n",
    "print(\"\\nRESUMEN DEL MODELO COMPLETO:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura muy usada en CNN\n",
    "\n",
    "NOTA: Veremos qué son las capas MaxPooling\n",
    "\n",
    "![VGG](https://viso.ai/wp-content/uploads/2024/04/vgg-16.bak-1280x708.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaneBW10p6JH"
   },
   "source": [
    "---\n",
    "\n",
    "Creado por **Guillermo Iglesias** (guillermo.iglesias@upm.es) y **Jorge Dueñas Lerín** (jorge.duenas.lerin@upm.es)\n",
    "\n",
    "<img src=\"https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png\">"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/gpu.ipynb",
     "timestamp": 1611939535453
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "M-todos-Generativos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
