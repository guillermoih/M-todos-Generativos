\input{Slides/include/preamble.tex}
\input{Slides/include/glossary.tex}
\input{Slides/include/data.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       
\title{Del Transformer Original al Ecosistema Moderno}

\begin{document}
\maketitle

\begin{frame}{Contenidos}
  \begin{enumerate}
      \item Introducción
      \item Auto-encoders (AEs)
      \item{Auto-encoders Variacionales (VAEs)}
      \item{Generative Adversarial Networks (GANs)}
      \item{Transformers}
      \item{\textbf{Del Transformer Original al Ecosistema Moderno}}
      \item{Diffusion Models}
    \end{enumerate}
\end{frame}

\section{Evolución y Variantes de Transformers}


\begin{frame}{Recap: ¿Por qué funcionó tan bien el transformer original?}

\begin{columns}
\column{0.6\textwidth}
\begin{itemize}
    \item \textbf{Atención}: mecanismo que facilita capturar dependencias a largo plazo sin recurrencia
    \item \textbf{Paralelización}: eliminación de dependencias secuenciales
    \item \textbf{Escalabilidad}: arquitectura capaz de aprovechar grandes cantidades de datos
    \item \textbf{Transferibilidad}: representaciones aprendidas útiles para múltiples tareas
\end{itemize}
\column{0.4\textwidth}
\vspace{0.2em}
\includegraphics[width=\textwidth]{Slides/figures/02_Metodos_Generativos/trans-arch.png}
\end{columns}

Estos principios han dado lugar a tres familias arquitectónicas principales...

\end{frame}

\begin{frame}{Las tres familias arquitectónicas}

\begin{figure}
    \centering
    \includegraphics[width=0.65\textwidth]{Slides/figures/02_Metodos_Generativos/trans-arch5.png}
    \caption{De este diseño original se derivan tres familias: los modelos que entienden (encoder-only), los que generan (decoder-only) y los que traducen de una representación a otra (encoder-decoder).}
\end{figure}
\vspace{-0.4cm}
\begin{itemize}
    \item \textbf{Encoder-only}: solo la parte izquierda (encoders apilados)
    \item \textbf{Decoder-only}: solo la parte derecha (decoders apilados), 
    \begin{itemize}
        \item sin cross-attention pero con masked-attention
    \end{itemize}
    \item \textbf{Encoder-decoder}: la arquitectura completa
\end{itemize}

\end{frame}



% ============================================================
% Fases de Entrenamiento
% ============================================================


\section{Fases entrenamiento, tareas y modelos de ejemplos}

\begin{frame}{Tareas: Upstream vs Downstream}

\textbf{Pretraining} (conocimiento general) → \textbf{Fine-tuning} (tarea específica)

\vspace{0.5em}
\textbf{Upstream Task (Tarea general)}
\begin{itemize}
    \item Objetivo \textbf{inicial} del entrenamiento del modelo base
    \item Grandes volúmenes de datos, principalmente \textbf{no etiquetados}
    \item Aprendizaje \textbf{auto-supervisado o con supervisión débil}
    \item El modelo adquiere patrones lingüísticos y conocimiento general del mundo
\end{itemize}

\vspace{0.3em}
\textbf{Downstream Task (Tarea específica)}
\begin{itemize}
    \item Tarea \textbf{final} o aplicación concreta
    \item Datos más limitados, normalmente \textbf{etiquetados}
    \item Aprendizaje \textbf{supervisado o basado en instrucciones}
    \item El modelo se adapta al dominio o tarea de interés
\end{itemize}

\end{frame}

\note{
En la práctica, el upstream corresponde a la fase de aprendizaje general (donde el modelo aprende a predecir texto, llenar huecos o relacionar tokens), mientras que el downstream se refiere a las adaptaciones o especializaciones: clasificación, resumen, QA, etc.

Entre ambos mundos hay técnicas intermedias como el continued pretraining o el instruction tuning.

Decimos ``principalmente no etiquetados'' y ``normalmente etiquetados'' porque hoy existen variantes que no encajan del todo en esas categorías: por ejemplo, el pretraining puede incluir datos con supervisión débil (como tareas contrastivas o de emparejamiento imagen-texto), y el fine-tuning puede realizarse con datos sintéticos o generados por modelos (instruction tuning, RLHF) en lugar de conjuntos etiquetados manualmente.
}


\begin{frame}{Tareas: otras estrategias}

\textbf{Continued pretraining / Domain Adaptation}
    \begin{itemize}
        \item Corpus de dominio específico sin etiquetar
        \item Ejemplo: textos médicos, legales, etc.
        \item Mismo \textbf{tipo} de objetivo: aprender más sobre el lenguaje pero con datos especializados
        \item Adapta vocabulario y conocimiento del dominio
    \end{itemize}

\textbf{Aprendizaje sobre las tareas en el \textbf{contexto} (In-context learning)}
\begin{itemize} 
    \item Grandes modelos del lenguaje
    \item Grandes ventanas de contexto
    \item Aprender de la tarea sin modificar pesos.
    \item Ejemplo: zero-shot, few-shot, etc.
\end{itemize}

\textbf{Generación Aumentada por Recuperación (\textbf{RAG})}
\begin{itemize}
    \item Vectorización de conocimiento
    \item Inyección en el contexto
\end{itemize}

\end{frame}



\begin{frame}{Taxonomía de Tareas: Auto-supervisadas (sin etiquetas)}

\textbf{Sin etiquetas específicas:}

\begin{description}
    \item[MLM] \textit{Masked Language Modeling}
    \item[-] Predecir tokens enmascarados usando contexto bidireccional. 
    \item[-] \textbf{Ejemplo:} "El gato está \_\_\_ la alfombra" → "sobre".
    
    \item[CLM] \textit{Causal Language Modeling}
    \item[-]Predecir el siguiente token (autoregresivo).  
    \item[-]\textbf{Ejemplo:} "El clima está" → "soleado".
    
    \item[NSP] Next Sentence Prediction — predecir si una oración sigue a otra en el texto original.
    \item[-] \textbf{Ejemplo:} 
    \begin{itemize}
        \item A: "Juan salió de casa."
        \item B: "Luego compró pan."
        \item Etiqueta: Verdadero (sí son consecutivas).
    \end{itemize}
\end{description}

\end{frame}


\begin{frame}{Taxonomía de Tareas: Auto-supervisadas (sin etiquetas)}

\begin{description}

    \item[SOP] \textit{Sentence Order Prediction}
    \item[-] Determinar el orden correcto de oraciones.  
    \item[-] \textbf{Ejemplo:} "Juan salió. Luego compró pan." vs "Luego compró pan. Juan salió."
    
    \item[RTD] \textit{Replaced Token Detection}
    \item[-] Detectar tokens falsos (ELECTRA).  
    \item[-] \textbf{Ejemplo:} "El perro comió pizza" → detectar que "pizza" es improbable.
    
    \item[Span Corruption]  
    \item[-] Predecir secuencias enmascaradas.  
    \item[-] \textbf{Ejemplo:} "El \_\_\_ está \_\_\_ la mesa" → "libro", "sobre".
\end{description}

\end{frame}



\begin{frame}{Taxonomía de Tareas: Supervisadas (Fine-tuning)}

\textbf{Con etiquetas específicas:}

\begin{description}
    \item[Text Classification] Categorizar texto completo.  
    \item[-] \textbf{Ejemplo:} "Este producto es excelente"
    \item[-] Sentimiento: positivo.
    
    \item[Token Classification] Etiquetar cada token.  
    \item[-] \textbf{Ejemplo NER:} "Barack Obama nació en Hawái"
    \item[-] [Persona: Barack Obama], [Lugar: Hawái].
    \item[-] \textbf{Ejemplo POS}: Part-of-Speech tagging (sustantivos, verbos, etc.)
     
    \item[Question Answering] Responder preguntas basadas en contexto.  
    \item[-] \textbf{Ejemplo extractivo:}
    \item[-] "Obama fue el presid... nació en Hawái, en la ... ¿Dónde nació Obama?"
    \item[-] Respuesta: "Hawái".
\end{description}

\end{frame}


\begin{frame}{Taxonomía de Tareas: Generación y Transformación}

\begin{description}
    \item[Summarization] Resumir texto largo.  
    \item[-] \textbf{Ejemplo:} Artículo
    \item[-] "Resumen breve".

    
    \item[Paraphrasing] Reformular manteniendo significado.  
    \item[-] \textbf{Ejemplo:} "Hace frío"
    \item[-] "La temperatura es baja".
    
    \item[Dialogue] Responder en conversación.  
    \item[-] \textbf{Ejemplo:} Usuario: "¿Cómo estás?"
    \item[-] Modelo: "Muy bien, gracias".

    \item[Translation] Traducir idiomas.  
    \item[-] \textbf{Ejemplo:} "Hello world"
    \item[-] "Hola mundo".
\end{description}

\end{frame}






\begin{frame}{Tareas: resumen}

\textbf{Upstream Tasks (Pretraining – aprendizaje general)}
\begin{itemize}
  \item \textbf{Auto-supervisadas:} MLM, CLM, NSP, SOP, RTD, Span Corruption
  \item Aprender representaciones y conocimiento general del lenguaje
\end{itemize}

\textbf{Continued Pretraining / Domain Adaptation}
\begin{itemize}
  \item \textbf{Auto-supervisadas:} sobre corpus especializado.
  \item Ejemplo: GPT -> Generamos LegalGPT o MedicalGPT
\end{itemize}

\textbf{Downstream Tasks (Fine-tuning o uso contextual)}
\begin{itemize}
  \item \textbf{Supervisadas:} Text / Token Classification, QA, Summarization, Paraphrasing, Dialogue, Translation
  \item \textbf{Sin actualización de pesos:} In-context Learning (zero-shot, few-shot)
\end{itemize}

\textbf{Estrategias híbridas:}
\begin{itemize}
    \item RAG (Retrieval-Augmented Generation)
\end{itemize}

\end{frame}


\begin{frame}{Tareas: resumen}


\begin{block}{Analogía educativa}

Pensemos en el modelo como una persona que aprende. \\[0.5em]

\textbf{Pretraining:} la etapa escolar. Aprende a leer, escribir y el uso del lenguaje.  
No hay tareas concretas, solo comprensión general del lenguaje y del mundo. \\[0.5em]

\textbf{Domain adaptation:} la universidad. Se especializa en un campo (medicina, derecho, programación)
Aprende con textos del dominio, pero sigue sin etiquetas. \\[0.5em]

\textbf{Fine-tuning:} el trabajo. Aprende una tarea concreta con ejemplos reales.  
Ya no estudia teoría: aplica lo aprendido para resolver problemas específicos. \textbf{Ejemplos:} un médico que aprende a diagnosticar, o un abogado que redacta demandas

\end{block}
\end{frame}


\section{Ejemplos}


\begin{frame}{Familia 1: Encoder-only — BERT}

\textbf{BERT}: \textit{Bidirectional Encoder Representations from Transformers}

\textbf{Características principales}
\begin{itemize}
    \item \textbf{Arquitectura:} sólo usa \textit{encoders} del Transformer
    \item \textbf{Bidireccionalidad:} cada token ve todo el contexto (izquierda y derecha)
    \item \textbf{Objetivo de entrenamiento:} 
    \textit{Masked Language Modeling (MLM)} y \textit{Next Sentence Prediction (NSP)}
    \item Produce \textbf{representaciones contextuales} ricas
\end{itemize}

\centering
\includegraphics[width=0.75\textwidth]{Slides/figures/llm/bert.png}

\end{frame}


\begin{frame}{Entrenamiento de BERT (Pre-training)}

\textbf{Objetivo:} aprender representaciones lingüísticas generales a partir de grandes corpus sin supervisión.

\textbf{Datos de entrenamiento}: \textit{BooksCorpus} y \textit{Wikipedia} en inglés

\textbf{Tareas de pre-entrenamiento}
\begin{itemize}
    \item Ambas cabezas (MLM y NSP) se entrenan en paralelo $L = L_{\text{MLM}} + L_{\text{NSP}}$

    \item \textbf{Masked Language Modeling (MLM)}  
    \begin{itemize}
        \item Se selecciona aleatoriamente el \textbf{15\% de los tokens} para intentar predecirlos.
        \item De esos tokens seleccionados:
        \begin{itemize}
            \item 80\% se reemplazan por \texttt{[MASK]}
            \item 10\% se reemplazan por un token aleatorio
            \item 10\% se dejan sin modificar
        \end{itemize}
    \end{itemize}
    \item \textbf{Next Sentence Prediction (NSP)}  
    \begin{itemize}
        \item 50\% de los pares son oraciones consecutivas (\textit{IsNext})
        \item 50\% son oraciones no relacionadas (\textit{NotNext})
    \end{itemize}
\end{itemize}

\end{frame}



\begin{frame}{Representaciones y entrenamiento de BERT}

\textbf{El token [CLS]} Se añade al inicio de la secuencia
\begin{itemize}
    \item Su vector final \textbf{resume el significado global} del texto.
    \item Se usa para tareas de clasificación (\textit{NSP}, análisis de sentimiento, etc.).
\end{itemize}

\textbf{Embeddings por token}
\begin{itemize}
    \item Cada token obtiene una representación contextual completa.
    \item Útiles para tareas de secuencia (NER, QA, POS tagging).
\end{itemize}


\begin{block}{Ejemplo}
\vspace{0.2cm}
\small
\texttt{Input: "El [MASK] duerme. Está [MASK]."}\\
\texttt{ Objetivo MLM: predecir "gato", "cansado"}\\[0.3em]
\texttt{ Objetivo NSP: IsNext}\\
\vspace{0.2cm}
\texttt{Input: "El [MASK] duerme. Los trenes son [MASK]."}\\
\texttt{ Objetivo MLM: predecir "gato", "rápidos"}\\[0.3em]
\texttt{ Objetivo NSP: NotNext}\\
\end{block}

\end{frame}



\begin{frame}{Familia 2: Decoder-only — GPT-2}

\textbf{GPT-2}: \textit{Generative Pre-trained Transformer 2}

\begin{columns}[T]
\column{0.55\textwidth}

\textbf{Características principales}
\begin{itemize}
    \item Usa solo la parte \textbf{decoder} del Transformer.
    \item Entrenamiento \textbf{autoregresivo}: predice el siguiente token dado el contexto previo.
    \item Objetivo de entrenamiento: \textbf{Causal Language Modeling (CLM)}.
    \item Atención \textbf{unidireccional}: cada token solo ve los anteriores.
\end{itemize}

\column{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{Slides/figures/llm/gpt2.png}
\href{https://medium.com/@vipul.koti333/from-theory-to-code-step-by-step-implementation-and-code-breakdown-of-gpt-2-model-7bde8d5cecda}{Link a fuente de imagen}
Masked MHA
\end{columns}

\vspace{0.5cm}

\textbf{GPT-2}: \textit{¡Ojo! se entrena en paralelo. En inferencia se va token a token.}

\end{frame}



\begin{frame}{Generación con GPT-2}

\textbf{Objetivo:} generar texto coherente y fluido, un token a la vez.

\textbf{Durante la generación}
\begin{itemize}
    \item El modelo calcula la probabilidad de cada token posible:
    \[
    P(x_t | x_{<t})
    \]
    \item Se elige el siguiente token según una estrategia de muestreo.
    \item El proceso continúa hasta alcanzar un token de parada o el límite de longitud.
\end{itemize}

\textbf{Estrategias de muestreo}
\begin{itemize}
    \item \textbf{Greedy}: elige siempre el token más probable (poco creativo).
    \item \textbf{Sampling}: elige aleatoriamente según la distribución de probabilidad.
    \item \textbf{Top-$k$ / Nucleus (Top-$p$)}: limita el muestreo a los tokens más probables.
\end{itemize}

\end{frame}


\begin{frame}{Generación con GPT-2}

\begin{block}{Ejemplo de generación}
\small
\texttt{Input: "El gato duerme en"} $\rightarrow$ \texttt{"el sofá."}\\[0.3em]
\texttt{Input: "La IA transformará"} $\rightarrow$ \texttt{"la educación en el futuro."}
\end{block}

\vspace{0.4em}

\begin{block}{Temperatura y creatividad}
\small
Controla la aleatoriedad en el muestreo:\\
\[
P_i' = \frac{\exp(\log P_i / T)}{\sum_j \exp(\log P_j / T)}
\]
\textbf{Temperatura baja} $(T<1)$ → texto predecible\\
\textbf{Temperatura alta} $(T>1)$ → texto más variado
\end{block}
\textbf{Casos de uso}:
\begin{itemize}
    \item Traducción automática
    \item Resumen de textos
    \item Paráfrasis
    \item Question Answering generativo
\end{itemize}

\end{frame}




\begin{frame}{Familia 3: Encoder–Decoder - T5}

\textbf{T5}: \textit{Text-To-Text Transfer Transformer}

\textbf{Características principales}:
\begin{itemize}
    \item \textbf{Arquitectura completa}: encoder bidireccional + decoder autoregresivo.
    \item Entrenamiento basado en \textbf{denoising}: el modelo aprende a reconstruir texto corrupto.
    \item Formulación unificada: todo se expresa como una tarea \textbf{text-to-text}.
    \item Máxima \textbf{flexibilidad} para tareas de tipo seq2seq.
\end{itemize}

\end{frame}



\begin{frame}{Familia 3: Encoder–Decoder - T5}

\centering
\includegraphics[width=1\textwidth]{Slides/figures/llm/t5.png}

\textbf{Casos de uso}:
\begin{itemize}
    \item Traducción automática
    \item Resumen de textos
    \item Paráfrasis y reformulación
    \item Question Answering generativo
\end{itemize}

\end{frame}



\begin{frame}{Comparación de las tres familias}

\begin{table}[]
\centering
\small
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Familia} & \textbf{Atención} & \textbf{Objetivo} & \textbf{Uso principal} \\ \hline
Encoder-only & Bidireccional & MLM & Comprensión \\ \hline
Decoder-only & Causal & CLM & Generación \\ \hline
Encoder-decoder & Ambas & Seq2seq & Transformación \\ \hline
\end{tabular}
\end{table}

\vspace{1em}

\begin{block}{Regla general}
\begin{itemize}
    \item ¿Necesitas \textbf{entender}? → Encoder-only
    \item ¿Necesitas \textbf{generar}? → Decoder-only
    \item ¿Necesitas \textbf{transformar}? → Encoder-decoder
\end{itemize}
\end{block}

\end{frame}



\section{Por organizar}






















% ============================================================
% SLIDE 6: Ejemplos Concretos de Cada Fase
% ============================================================

\begin{frame}{Ejemplos Concretos del Pipeline}

\begin{table}[]
\centering
\small
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Modelo} & \textbf{Pretraining} & \textbf{Domain Adapt.} & \textbf{Fine-tuning} \\ \hline
BERT & 
\begin{tabular}[c]{@{}l@{}}Wikipedia +\\BookCorpus\\(MLM + NSP)\end{tabular} & 
- & 
\begin{tabular}[c]{@{}l@{}}Sentiment\\classification\end{tabular} \\ \hline

BioBERT & 
\begin{tabular}[c]{@{}l@{}}Igual que\\BERT\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}PubMed +\\PMC\\(MLM)\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}Biomedical\\NER\end{tabular} \\ \hline

GPT-3 & 
\begin{tabular}[c]{@{}l@{}}Common Crawl\\+ libros\\(CLM)\end{tabular} & 
- & 
\begin{tabular}[c]{@{}l@{}}Few-shot\\(sin fine-tune)\end{tabular} \\ \hline

LegalBERT & 
\begin{tabular}[c]{@{}l@{}}Igual que\\BERT\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}Legal texts\\(MLM)\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}Contract\\classification\end{tabular} \\ \hline
\end{tabular}
\end{table}

\vspace{0.5em}

\begin{block}{Nota sobre GPT-3}
Los modelos muy grandes pueden hacer muchas tareas sin fine-tuning explícito (in-context learning), pero el concepto de fases sigue aplicando a su entrenamiento inicial.
\end{block}

\end{frame}

% ============================================================
% SLIDE 7: Decisión: ¿Qué Fase Necesitas?
% ============================================================

\begin{frame}{Guía de Decisión: ¿Qué Necesitas Entrenar?}

\begin{block}{¿Tienes un modelo preentrenado de propósito general?}
\textbf{SÍ} → Evalúa si hacer domain adaptation o ir directo a fine-tuning
\end{block}

\vspace{0.5em}

\textbf{¿Cuándo hacer DOMAIN ADAPTATION?}
\begin{itemize}
    \item Tu dominio tiene vocabulario muy específico (médico, legal, técnico)
    \item Tienes muchos datos de dominio sin etiquetar (> 1M documentos)
    \item El modelo general falla en conceptos básicos de tu dominio
    \item Vas a hacer múltiples tareas en ese dominio
\end{itemize}

\vspace{0.5em}

\textbf{¿Cuándo ir directo a FINE-TUNING?}
\begin{itemize}
    \item Tu dominio es cercano al lenguaje general
    \item Tienes pocos datos de dominio
    \item Solo vas a hacer una tarea específica
    \item El modelo general ya funciona razonablemente bien
\end{itemize}

\end{frame}




\section{Conceptos Clave del Ecosistema}



\begin{frame}{Paradigmas de Aprendizaje}

\textbf{1. Transfer Learning: Pretraining + Fine-tuning}
\begin{itemize}
    \item Entrenamiento en dos fases
    \item Modifica los parámetros del modelo
    \item Requiere dataset de la tarea objetivo
    \item Ejemplo: BERT pre-entrenado → fine-tuned para clasificación de sentimiento
\end{itemize}

\vspace{0.5em}

\textbf{2. In-Context Learning (emergente en modelos grandes)}
\begin{itemize}
    \item \textbf{No modifica} los parámetros del modelo
    \item Aprende de los ejemplos en el prompt
    \item Ventana grande, conocimiento amplio
    \item Solo disponible en modelos suficientemente grandes
\end{itemize}

\end{frame}

\begin{frame}{In-Context Learning: Zero-shot y Few-shot}

\textbf{Zero-shot}: Sin ejemplos
\begin{block}{}
\small
\texttt{Classify the sentiment of this text: "I love this product!"}

\texttt{Answer: Positive}
\end{block}

\vspace{0.5em}

\textbf{Few-shot}: Con pocos ejemplos (2-5 típicamente)
\begin{block}{}
\small
\texttt{Classify sentiment:}

\texttt{Text: "Great service!" → Positive}

\texttt{Text: "Terrible experience." → Negative}

\texttt{Text: "I love this!" → ?}
\end{block}

\vspace{0.5em}

\begin{alertblock}{Diferencia clave}
In-context learning NO actualiza parámetros, solo usa el contexto del prompt.
\end{alertblock}

\end{frame}

\begin{frame}{Prompt Engineering}

\begin{itemize}
    \item \textbf{Disciplina} que surgió con el auge de los LLMs
    \item Técnicas principales:
    \begin{itemize}
        \item \textbf{Instruction prompting}: instrucciones claras y específicas
        \item \textbf{Few-shot prompting}: proporcionar ejemplos
        \item \textbf{Chain-of-thought}: pedir razonamiento paso a paso
        \item \textbf{Role prompting}: asignar un rol al modelo
    \end{itemize}
    \item Arte y ciencia de diseñar instrucciones efectivas
    \begin{itemize}
        \item CoT -> Modelos razonadores
    \end{itemize}
    \item El prompt adecuado puede marcar la diferencia entre éxito y fracaso
\end{itemize}



\end{frame}





\begin{frame}{Chain of Thought (CoT) y Modelos razonadores}

\small
\textbf{Chain of Thought:} Pedir al modelo que piense y haga explícito el razonamiento intermedio de una respuesta/tarea.  

\begin{itemize}
    \item \textbf{Resultado:} En lugar de dar directamente la respuesta final, el modelo genera una secuencia de pasos lógicos antes de llegar a la solución.
    \item \textbf{Ejemplo:} Para resolver un problema matemático, el modelo escribe los cálculos y justificaciones antes de dar el resultado.
    \item \textbf{Objetivo:} Mejorar la precisión en tareas complejas (matemáticas, lógica, planificación) al permitir que el modelo “razone” de forma estructurada.
\end{itemize}

\textbf{Modelos razonadores:} Modelos diseñados y optimizados específicamente para razonar de forma más profunda y fiable. \textbf{Técnicas empleadas:}
\begin{itemize}
    \item \textbf{Entrenamiento con CoT:} aprenden a generar pasos intermedios.
    \item \textbf{Self-Consistency:} generan varias cadenas de razonamiento y eligen la más coherente.
    \item \textbf{Verificación interna:} el modelo revisa sus propios pasos antes de responder.
\end{itemize}

\end{frame}


\section{Herramientas}

\begin{frame}{Hugging Face Hub: El GitHub de los modelos}

\begin{columns}
\column{0.5\textwidth}
\textbf{¿Qué es Hugging Face Hub?}
\begin{itemize}
    \item Plataforma centralizada para compartir:
    \begin{itemize}
        \item Modelos pre-entrenados (180,000+)
        \item Datasets
        \item Spaces (demos interactivas)
    \end{itemize}
    \item \textbf{Model cards}: documentación estandarizada
    \begin{itemize}
        \item Arquitectura y tamaño
        \item Dataset de entrenamiento
        \item Métricas de rendimiento
        \item Limitaciones conocidas
        \item Consideraciones éticas
    \end{itemize}
\end{itemize}

\column{0.5\textwidth}
\textbf{Biblioteca transformers}
\begin{itemize}
    \item Abstracción unificada
    \item Mismo código para todos los modelos
    \item API simple:
\end{itemize}

\begin{block}{}
\tiny
\texttt{from transformers import pipeline}

\texttt{classifier = pipeline("sentiment-analysis")}

\texttt{result = classifier("I love this!")}
\end{block}

\end{columns}

\end{frame}

\begin{frame}{Ollama: LLMs Localmente}

\begin{columns}
\column{0.6\textwidth}
\textbf{¿Qué es Ollama?}
\begin{itemize}
    \item Herramienta para ejecutar LLMs en tu máquina
    \item Interfaz simple tipo Docker
    \item Optimizado para inferencia local
    \item Modelos disponibles: LLaMA, Mistral, Phi, etc.
\end{itemize}

\vspace{0.5em}

\textbf{Ventajas}:
\begin{itemize}
    \item Privacidad (datos no salen de tu máquina)
    \item Sin costes de API
    \item Control completo
\end{itemize}

\column{0.4\textwidth}
\begin{block}{Uso básico}
\small
\texttt{\$ ollama run llama2}

\vspace{0.3em}
\texttt{>>> Write a poem}

\texttt{[respuesta del modelo]}
\end{block}

\vspace{0.5em}

\textbf{Requisitos}:
\begin{itemize}
    \item GPU recomendada
    \item Suficiente RAM/VRAM
    \item Modelos de 7B+ requieren ≥16GB
\end{itemize}

\end{columns}

\end{frame}



\begin{frame}{LangChain: Orquestando el Razonamiento de los LLMs}

\textbf{¿Qué es LangChain?}
\begin{itemize}
    \item Framework para construir aplicaciones que usan LLMs de forma estructurada.
    \item Permite conectar modelos con:
    \begin{itemize}
        \item Fuentes de datos externas (APIs, bases de datos, documentos)
        \item Memoria y contexto persistente
        \item Herramientas externas (búsquedas, cálculos, acciones)
    \end{itemize}
\end{itemize}

\vspace{0.5em}

\textbf{Componentes principales:}
\begin{itemize}
    \item \textbf{Prompt Templates}: diseño sistemático de prompts.
    \item \textbf{Chains}: secuencias de llamadas a modelos o funciones.
    \item \textbf{Agents}: modelos con capacidad de decidir qué acción tomar.
    \item \textbf{Memory}: almacenamiento del contexto de conversación.
\end{itemize}

\vspace{0.5em}

\begin{block}{Ejemplo básico}
\tiny
\texttt{from langchain import PromptTemplate, LLMChain}\\
\texttt{from langchain.llms import Ollama}\\[0.3em]
\texttt{llm = Ollama(model="llama2")}\\
\texttt{prompt = PromptTemplate.from_template("Resume: \{text\}")}\\
\texttt{chain = LLMChain(llm=llm, prompt=prompt)}\\
\texttt{print(chain.run(text="LangChain permite..."))}
\end{block}

\vspace{0.5em}

\textbf{Objetivo:} Pasar de simples respuestas de texto a flujos de razonamiento controlados, conectados y reproducibles.

\end{frame}

\begin{frame}{Consideraciones Prácticas}

\textbf{1. Escalabilidad}
\begin{itemize}
    \item \textbf{Distribución}: entrenamiento multi-GPU, multi-nodo
    \item \textbf{Mixture of Experts (MoE)}: activar solo partes del modelo
    \item \textbf{Paralelismo}: de datos, de modelo, de pipeline
\end{itemize}

\vspace{0.5em}

\textbf{2. Trade-offs importantes}
\begin{itemize}
    \item \textbf{Tamaño vs. Rendimiento}: modelos más grandes ≠ siempre mejor
    \item \textbf{Calidad vs. Coste}: GPT-4 es mejor pero más caro que GPT-3.5
    \item \textbf{Latencia vs. Throughput}: batch size afecta ambos
    \item \textbf{Precisión vs. Memoria}: FP32 vs FP16 vs INT8
\end{itemize}

\vspace{0.5em}

\begin{alertblock}{Regla de oro}
Empieza con modelos pequeños, escala solo si es necesario.
\end{alertblock}

\end{frame}

\begin{frame}{Técnicas de Optimización}

\begin{columns}
\column{0.5\textwidth}
\textbf{Reducción de tamaño}
\begin{itemize}
    \item \textbf{Quantization}: FP16, INT8, INT4
    \item \textbf{Pruning}: eliminar pesos poco importantes
    \item \textbf{Distillation}: entrenar modelo pequeño que imita uno grande
    \item Ejemplo: DistilBERT (40\% más pequeño, 60\% más rápido)
\end{itemize}

\column{0.5\textwidth}
\textbf{Optimización de inferencia}
\begin{itemize}
    \item \textbf{KV-cache}: cachear atención en generación
    \item \textbf{Batching}: procesar múltiples inputs juntos
    \item \textbf{Model compilation}: TensorRT, ONNX
    \item \textbf{Speculative decoding}: generar múltiples tokens a la vez
\end{itemize}

\end{columns}

\vspace{1em}

\begin{exampleblock}{Impacto}
Con cuantización a INT8: 4x menos memoria, 2-4x más rápido, pérdida mínima de calidad.
\end{exampleblock}

\end{frame}

\section{Limitaciones y Retos}

\begin{frame}{Problemas Técnicos}

\textbf{1. Coste Computacional y Memoria}
\begin{itemize}
    \item GPT-3 (175B parámetros): \$4.6M en cloud compute
    \item Inference: modelos grandes requieren múltiples GPUs
    \item Soluciones: quantization, distillation, MoE
\end{itemize}

\vspace{0.5em}

\textbf{2. Hallucinations (Alucinaciones)}
\begin{itemize}
    \item Generación de información \textbf{falsa pero plausible}
    \item Especialmente problemático en datos fuera de distribución
    \item Mitigaciones: RAG (Retrieval-Augmented Generation), fine-tuning, prompt engineering
\end{itemize}

\vspace{0.5em}

\textbf{3. Catastrophic Forgetting}
\begin{itemize}
    \item Al fine-tunear, el modelo puede "olvidar" conocimiento previo
    \item Soluciones: regularización, continual learning, adapter layers
\end{itemize}

\end{frame}

\begin{frame}{Problemas Éticos y Sociales}

\textbf{1. Bias en Datos de Entrenamiento}
\begin{itemize}
    \item Los modelos aprenden (y amplifican) sesgos de los datos
    \item Ejemplos: género, raza, religión, nacionalidad
    \item Importante: evaluar bias antes de deployment
\end{itemize}

\vspace{0.5em}

\textbf{2. Consideraciones de Uso Responsable}
\begin{itemize}
    \item \textbf{Transparencia}: documentar limitaciones (model cards)
    \item \textbf{Privacidad}: datos de entrenamiento pueden memorizar información sensible
    \item \textbf{Dual use}: potencial para desinformación, spam, phishing
    \item \textbf{Impacto ambiental}: huella de carbono del entrenamiento
\end{itemize}

\vspace{0.5em}

\begin{alertblock}{Responsabilidad}
Como desarrolladores, tenemos la responsabilidad de usar estas tecnologías de forma ética.
\end{alertblock}

\end{frame}

\begin{frame}{Futuro y Tendencias}

\textbf{Tendencias actuales (2024-2025)}:
\begin{itemize}
    \item \textbf{Modelos más eficientes}: Mistral, Phi, Gemma (pequeños pero capaces)
    \item \textbf{Multimodalidad}: GPT-4V, LLaVA (texto + imágenes)
    \item \textbf{Long context}: modelos con contexto de 100K+ tokens
    \item \textbf{Agents}: LLMs que pueden usar herramientas y planificar
    \item \textbf{Open source}: democratización (LLaMA, Falcon, Mistral)
\end{itemize}

\vspace{0.5em}

\textbf{Retos abiertos}:
\begin{itemize}
    \item Razonamiento complejo y matemático
    \item Consistencia factual
    \item Personalización eficiente
    \item Sostenibilidad
\end{itemize}

\end{frame}

\begin{exercise}
\href{https://colab.research.google.com/}{Práctica: Explorando el Ecosistema de Transformers}

En este notebook exploraremos:
\begin{itemize}
    \item Las tres familias de transformers (encoder-only, decoder-only, encoder-decoder)
    \item In-context learning: zero-shot vs few-shot
    \item Hugging Face Hub: buscar y usar modelos
    \item Comparar diferentes modelos y tareas
\end{itemize}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Recursos}
\begin{itemize}
    \item Hugging Face Hub: \url{https://huggingface.co/models}
    \item Transformers Documentation: \url{https://huggingface.co/docs/transformers}
    \item Ollama: \url{https://ollama.ai/}
    \item The Illustrated Transformer: \url{http://jalammar.github.io/illustrated-transformer/}
    \item Prompt Engineering Guide: \url{https://www.promptingguide.ai/}
\end{itemize}
\end{frame}

\appendix

\begin{frame}<presentation:0>{License}
    \begin{block}{Tema \texttt{slides-upm}. Puedes obtener sus fuentes en}
        \begin{center}\url{http://gitlab.com/blazaid/slides-upm}\end{center}
    \end{block}
  
    Tanto esta presentación como el tema están licenciados bajo \href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons
  Atribución-CompartirIgual 4.0 Internacional (CC BY-SA 4.0)}.
    \begin{center}\ccbysa\end{center}
\end{frame}

\end{document}
