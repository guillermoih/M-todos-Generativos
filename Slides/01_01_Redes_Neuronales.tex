\input{Slides/include/preamble.tex}
\input{Slides/include/glossary.tex}
\input{Slides/include/data.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       
\title{Repaso de redes neuronales artificiales}

% Definir color de enlace
\definecolor{linkblue}{RGB}{0,0,180} % un azul visible pero agradable

% Configurar hyperref
\hypersetup{
    colorlinks=true,  % los enlaces tendrán color
    linkcolor=linkblue,  % enlaces internos (índice, secciones)
    urlcolor=linkblue,   % enlaces externos (href)
    citecolor=linkblue
}

\begin{document}
\maketitle

\section{Repaso de conceptos básicos de redes neuronales}

\begin{frame}{Redes neuronales: concepto general}
Una \alert{red neuronal} es un sistema matemático capaz de aprender ha realizar predicciones a partir de unos datos de entrada, fue propuesta por McCulloch y Pitts en 1943\cite{mcculloch1943logical}, se basaba en \alert{imitar} el comportamiento de una neurona biológica.

\begin{itemize}
    \item Toma ciertos \alert{estímulos} de entrada, los procesa y genera una nueva salida.
    \item En el caso biológico los estímulos provienen de \alert{impulsos nerviosos}, mientras que en el caso de las neuronas artificiales esto es replicado a través de cálculos matemáticos.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Slides/figures/Tema 3/Neurona_Biologica_Artificial.png}
\end{figure}    
\end{frame}

\begin{frame}{Grandes hitos en redes neuronales}
\footnotesize{
Las redes neuronales han evolucionado a través de hitos clave que han marcado su desarrollo:

\begin{itemize}
    \item \alert{1943: Neurona artificial} (McCulloch y Pitts): Primer modelo matemático inspirado en la biología
    \item \alert{1958: Perceptrón} (Rosenblatt): Uso de funciones de activación como el umbral
    \item \alert{1986: Retropropagación} (Rumelhart et al.): Entrenamiento efectivo de redes profundas
    \item \alert{1989: CNNs} (LeCun et al.): Redes convolucionales para visión por computador
    \item \alert{Inviernos de la IA}: Estancamiento. Límites técnicos/expectativas no cumplidas
    \item \alert{2012: AlexNet}: \textbf{GPUs} y big data impulsan el aprendizaje profundo moderno
    \item \alert{2017: Transformers} (Vaswani et al.): Nueva arquitectura para NLP.
    \item \alert{2020s: Difusión}: Modelos generativos para imágenes y datos sintéticos
    \item \alert{Actualidad}: Grandes modelo multimodales
    \item \alert{Futuro}: ¿Invierno o Inteligencia artificial general?
\end{itemize}
}
\end{frame}

\begin{frame}{Redes neuronales: concepto general}
Una \alert{neurona artificial} realiza cálculos matemáticos para transformar ciertos valores numéricos.
Para dicha labor, existen diversos \alert{elementos} dentro de una neurona artificial:
\begin{columns}[c]
\centering
\begin{column}{0.7\textwidth}
    \begin{itemize}
    \item \alert{Entradas} ($ x_{i} $): Introducen valores numéricos en la neurona
    \item \alert{Salida} ($ y $): Suministra la salida de la neurona
    \item \alert{Pesos} ($ w_{i} $): Son parámetros capaces de cambiar, realizan el aprendizaje de la neurona
    \item \alert{Bias} ($ b $): Realiza la misma función que cualquier otro peso, pero el valor de su entrada es \alert{siempre} 1
    \item \alert{Función de activación} ($ a $): Es una función que participa en el cálculo de la salida de la neurona
    \end{itemize}
\end{column}
\begin{column}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Tema 3/Neurona_Componentes.png}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Redes neuronales: bias}
El \alert{bias} (sesgo en Español) permite que la red neuronal pueda ajustar mejor sus predicciones. 
\begin{columns}[c]
\centering
\begin{column}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/bias-no.png}
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/bias-si.png}
\end{column}
\begin{column}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/bias-no-data.png}
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/bias-si-data.png}
\end{column}
\end{columns}

\scriptsize{
Fuente izq: https://www.pico.net/kb/the-role-of-bias-in-neural-networks/
}

\end{frame}

\begin{frame}{Redes neuronales: activación}
Capas de activación. En salida.
\newline

\begin{columns}[c]
\centering
\begin{column}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/clasificación.png}
    \scriptsize{Fuente Coursera}
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/sigmoid_graph.png}
    \scriptsize{Wikipedia}
\end{column}
\begin{column}{0.4\textwidth}
    \centering

    \textbf{Sigmoid}
    \begin{equation}
     y = \frac{1}{1 + e^{-z}}
    \end{equation}
    
    \vspace{0.5cm}
    
    \textbf{Ejemplo}
    \begin{equation}
     y = \frac{1}{1 + e^{-(3*x-2)}}
    \end{equation}

    \vspace{0.5cm}
    
    \href{https://www.desmos.com/calculator/ctrsgcjd8n}{Calculadora gráfica}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Redes neuronales: activación}
Capas de activación. No linealidad.

\begin{columns}[c]
\centering
\begin{column}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/combilineal.png}
\end{column}
\begin{column}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/relu.png}
    \scriptsize{Wikipedia}
\end{column}
\begin{column}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/combinolineal.png}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Redes neuronales: activación}
Capas de activación. No linealidad. 3D

\begin{columns}[c]
\centering
\begin{column}{0.5\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{Slides/figures/Introduccion/activacion_3d.png}
\end{column}
\begin{column}{0.5\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{Slides/figures/Introduccion/activacion_3d_varias.png}
\end{column}
\end{columns}

\centering
\includegraphics[width=0.4\textwidth]{Slides/figures/Introduccion/activacion_3d_combi.png}

\scriptsize{DotCSV}

\end{frame}

\begin{frame}{Redes neuronales: entrenamiento}
El entrenamiento de las redes neuronales se puede dividir en tres fases: 
\begin{columns}[c]
\begin{column}{0.49\textwidth}
\begin{itemize}
    \item \alert{Predicción:} calculamos para las entradas la predicción de valor de salida de la red.
    \item \alert{Calculo de error:} con la predicción y el resultado que queremos obtener calculamos el error obtenido.
    \item \alert{Ajuste de pesos:} con el error se reajustan los parámetros de la red.
\end{itemize} 
\end{column}

\begin{column}{0.49\textwidth}
\begin{figure}
\centering
    \includegraphics[width=0.9\textwidth]{Slides/figures/Tema 3/jokeML.png}
\end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Algoritmo de propagación}
La fase de \alert{predicción} de una red neuronal se realiza a través del algoritmo de \alert{propagación}.

Este se encarga de procesar la \alert{entrada} y generar la \alert{salida} correspondiente.

Para ello, la computación de cada \alert{neurona} es la siguiente:
\begin{itemize}
    \item Cada entrada $x_{i}$ es \alert{multiplicada} por el valor de su peso correspondiente $w_{i}$.
    \item La entrada de \textit{bias} \alert{siempre} es \say{1}, y se multiplica por su peso correspondiente (a veces indicado como $w_{0}$).
    \item Todas las \alert{entradas} de la neurona se combinan haciendo una \alert{suma} de todas ellas, de tal manera que se realiza una \alert{combinación lineal}.
    \item El resultado de la combinación lineal se pasa por una \alert{función no lineal} para generar la \alert{salida} de la neurona.
\end{itemize}
\end{frame}

\begin{frame}{Algoritmo de propagación}
La ecuación que define este \alert{proceso} es la siguiente:
\setcounter{equation}{0}
\begin{equation}
    \Large f\left(\sum_{i=0}^{n} W_{i} X_{i}\right)
\end{equation}

\begin{figure}
\centering
    \includegraphics[width=0.6\textwidth]{Slides/figures/Tema 3/PropagationExample_1.png}
\end{figure}
\end{frame}

\begin{frame}{Algoritmo de propagación}
La ecuación que define este \alert{proceso} es la siguiente:
\setcounter{equation}{0}
\begin{equation}
    \Large f\left(\sum_{i=0}^{n} W_{i} X_{i}\right)
\end{equation}

\begin{figure}
\centering
    \includegraphics[width=0.6\textwidth]{Slides/figures/Tema 3/PropagationExample_2.png}
\end{figure}
\end{frame}

\begin{frame}{Algoritmo de propagación}
La ecuación que define este \alert{proceso} es la siguiente:
\setcounter{equation}{0}
\begin{equation}
    \Large f\left(\sum_{i=0}^{n} W_{i} X_{i}\right)
\end{equation}

\begin{figure}
\centering
    \includegraphics[width=0.6\textwidth]{Slides/figures/Tema 3/PropagationExample_3.png}
\end{figure}
\end{frame}

\begin{frame}{Algoritmo de propagación}
La ecuación que define este \alert{proceso} es la siguiente:
\setcounter{equation}{0}
\begin{equation}
    \Large f\left(\sum_{i=0}^{n} W_{i} X_{i}\right)
\end{equation}

\begin{figure}
\centering
    \includegraphics[width=0.6\textwidth]{Slides/figures/Tema 3/PropagationExample_4.png}
\end{figure}
\end{frame}

\begin{frame}{Algoritmo de propagación}
La ecuación que define este \alert{proceso} es la siguiente:
\setcounter{equation}{0}
\begin{equation}
    \Large f\left(\sum_{i=0}^{n} W_{i} X_{i}\right)
\end{equation}

\begin{figure}
\centering
    \includegraphics[width=0.8\textwidth]{Slides/figures/Tema 3/PropagationExample_5.png}
\end{figure}
\end{frame}

\begin{frame}{Funciones de activación}
Las \alert{funciones de activación} de cada neurona pueden variar, entre las más populares se encuentran:

\begin{figure}
\centering
    \includegraphics[width=\textwidth]{Slides/figures/Tema 3/Activations.png}
    \caption{\cite{Activations}}
\end{figure}
\end{frame}

\begin{frame}{Estructura de capas}
Una red de neuronas \say{estándar} se organiza por \alert{capas}, las cuales se componen por varias \alert{neuronas}.

Cada \alert{capa de neuronas} se conecta con la siguiente y recibe \alert{datos} de la anterior. De esta manera se produce el \alert{flujo de datos} a lo largo de la red.

\begin{figure}
\centering
    \includegraphics[width=0.8\textwidth]{Slides/figures/Tema 3/LayerStructure.png}
\end{figure}
\end{frame}

\begin{frame}{¿Por qué introducir más capas?}
Está matemáticamente \alert{demostrado} que sin función de activación las redes de neuronas sólo son capaces de resolver problemas \alert{linealmente separables}.

Esto es fácilmente demostrable, ya la computación de cada neurona corresponde con la ecuación de \alert{una recta}, y su combinación también.

\begin{figure}
\centering
    \includegraphics[width=0.5\textwidth]{Slides/figures/Tema 1/Separabilidad_Lineal.png}
\end{figure}

Por otra parte, K. Hornik, M. Stinchcombe, y H. White demostraron el 1985 que con \alert{una única capa oculta} las redes neuronales artificiales se convierten en aproximadores universales \cite{hornik1989multilayer}.
\end{frame}


\begin{frame}{Gradient descent y learning rate}
\alert{Descenso de gradiente}: algoritmo de optimización iterativo de primer orden que permite encontrar mínimos \alert{locales} en una función diferenciable. (Wikipedia).

¿Cómo funciona? Derivadas parciales para bajar la pendiente.


\begin{columns}[c]
\centering
\begin{column}{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/gradient.png}
\end{column}
\begin{column}{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/gradient_3d.png}
    ¡Ojo! pérdida no datos
\end{column}
\begin{column}{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Introduccion/descenso.png}
\end{column}
\end{columns}

\href{https://www.deeplearning.ai/ai-notes/optimization/index.html\#learning-rate}{Gradient descent un parámetro (learning rate)}

\href{https://www.deeplearning.ai/ai-notes/optimization/index.html\#adjusting-gradient-descent-hyperparameters}{Gradient descent w y b(batch size)} y \href{https://www.deeplearning.ai/ai-notes/optimization/index.html\#iterative-update}{Optimizadores}

\centering
\includegraphics[width=0.7\textwidth]{Slides/figures/Introduccion/lr.png}

\end{frame}

\begin{frame}{Algoritmo de retropropagación}
El algoritmo de \alert{retropropagación} o \alert{backpropagation} es el encargado de \alert{adaptar} la red de neuronas a su cometido específico.

Se basa en actualizar los \alert{pesos} de la red dependiendo del \alert{error} que esta haya tenido a la hora de predecir una \alert{salida} en concreto.

Con backpropagation obtendremos los \alert{gradientes (derivadas)} de la función de pérdida
para cada \alert{peso} de cada \alert{capa oculta}.

\begin{equation}
    \bigtriangleup w_i = w_i - \alpha \cdot (Error)
\end{equation}

donde $\alpha$ es el \alert{learning rate}, que define la \alert{magnitud} con la que la red realiza la \alert{actualización} de sus pesos.

\href{https://xnought.github.io/backprop-explainer/}{Backpropagation interactivo}

\end{frame}


\begin{frame}{Funciones de pérdida}
Existen múltiples métodos para calcular la \alert{distancia} de la \alert{predicción $\hat{y}$} con respecto de la \alert{salida deseada $y$}. Es decir, múltiples funciones de pérdida que nos permiten calcular el error.

\begin{equation}
    \textbf{MAE} = \frac{1}{n} \sum{\left | y - \hat{y} \right |}
\end{equation}

\begin{equation}
    \textbf{MSE} = \frac{1}{n}\sum{(y - \hat{y})^2}
\end{equation}

\begin{equation}
    \textbf{Binary Cross-Entropy} = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)
\end{equation} \href{https://www.desmos.com/calculator/hzgt6hyqr3}{Dibujo de la funciónlog.} Condicional if -> $y * ... + (1-y) * ...$

\begin{equation}
    \textbf{Cross-Entropy} = - \sum_{i=1}^{N}{y \cdot  \log \hat{y}}
\end{equation}
\end{frame}


\begin{frame}{Entrenamiento de redes neuronales}
Al realizar un entrenamiento con \alert{modelos de aprendizaje} se realiza una división del \alert{conjunto de datos} con el que se entrena. Este proceso ayuda a comprobar la \alert{fiabilidad} de la red.

\begin{figure}
\centering
    \includegraphics[width=0.6\textwidth]{Slides/figures/Tema 3/DatasetDivision.png}
\end{figure}
\end{frame}


\begin{frame}{Playground}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Slides/figures/Tema 3/Playground.png}
    \caption{\href{https://playground.tensorflow.org/}{Tensorflow playground}}
\end{figure}
\href{https://colab.research.google.com/github/guillermoih/M-todos-Generativos/blob/main/Notebooks/1.1_01-Playground.ipynb}{Ejercicios 1 y 2 completo. Ejercicio 3 introducción a siguientes conceptos.}
\end{frame}


\begin{frame}{Bias-variance tradeoff}


Existen dos \alert{conceptos fundamentales} que ayudan a analizar el rendimiento de una red neuronal:
\begin{itemize}
    \item \alert{Bias (sesgo)}: Error sistemático que aparece cuando el modelo es demasiado simple para capturar la complejidad de los datos. Se manifiesta en un alto error incluso en el conjunto de \alert{entrenamiento}.
    \item \alert{Variance (varianza)}: Error debido a la excesiva sensibilidad del modelo a los datos de entrenamiento. Se observa cuando existe una gran diferencia entre el error en \alert{entrenamiento} y el error en \alert{test}.
\end{itemize}

\begin{figure}
\centering
    \includegraphics[width=\textwidth]{Slides/figures/Tema 3/BiasVariance_1.jpg}
    \caption{\cite{BiasVariance_1}}
\end{figure}
\end{frame}

% OJO! No es necesario que se mueva mucho para tener alta varianza.

\begin{frame}{Bias-variance tradeoff}
\begin{figure}
\centering
    \includegraphics[width=0.7\textwidth]{Slides/figures/Tema 3/BiasVariance.png}
    \caption{\cite{BiasVariance}}
\end{figure}
\end{frame}

\begin{frame}{¿Cómo detectar alto bias o variance?}

\begin{columns}[c]
\begin{column}{0.49\textwidth}
\Large \alert{Alto bias}
\end{column}

\begin{column}{0.49\textwidth}
\Large \alert{Alto variance}
\end{column}
\end{columns}

\begin{columns}[c]
\begin{column}{0.49\textwidth}
\begin{itemize}
    \item Underfitting.
    \item Sobre-simplificación del problema.
    \item Valores de pérdida demasiado altos.
    \item Falla al capturar la tendencia de los datos.
\end{itemize} 
\end{column}

\begin{column}{0.49\textwidth}
\begin{itemize}
    \item Overfitting.
    \item Dataset demasiado ruidoso.
    \item Demasiada complejidad.
\end{itemize} 
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Esquema general de entrenamiento de redes neuronales}
\begin{figure}
\centering
    \includegraphics[width=0.9\textwidth]{Slides/figures/Tema 3/NNTrainingScheme.png}
\end{figure}
\end{frame}

\begin{frame}{Problemas del gradiente}
Los problemas \alert{derivados del gradiente} son comunes a todas las redes neuronales. Estos están \alert{directamente influenciados} por el número de capas de la red.

Se diferencian dos tipos:
\begin{itemize}
    \item Gradient explosion.
    \item Gradient vanishing.
\end{itemize}

Al realizarse la \alert{retropropagación} los \alert{valores de pérdida} pasan de unas capas a otras. En este algoritmo las derivadas de cada neurona pueden llegar a \alert{descontrolarse}.

\begin{equation}
    W_{x}^{\prime}=W_{x}-\mathrm{\alpha}\left(\frac{\partial \text {Loss}}{\partial W_{x}}\right)
\end{equation}
\end{frame}

\begin{frame}{Problemas del gradiente: Gradient explosion}
\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{Slides/figures/Tema 3/GradientExplosion.png}
    \caption{\cite{GradienExplosion}}
\end{figure}

El \alert{gradient explosion}, también conocido como \alert{exploding gradients} sucede cuando la actualización de pesos toma valores \alert{muy elevados}.

Se identifica con valores de pérdidas de \alert{NaN o muy exageradas}.
\end{frame}

\begin{frame}{Problemas del gradiente: Gradient Vanishing}
\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{Slides/figures/Tema 3/GradientVanishing.png}
    \caption{\cite{GradienVanishing}}
\end{figure}

Cuando sucede \alert{gradient vanishing}, también llamado \alert{vanishing gradients}, la actualización de pesos se hace \alert{nula} por tener valores \alert{muy pequeños}.

Se identifica cuando la pérdida es \alert{constante en el tiempo}.
\end{frame}

\begin{frame}{Origen de los problemas derivados del gradiente}
La principal \alert{causa} de estos problemas es usar \alert{funciones de activación} cuya derivada \alert{satura a 0}.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{Slides/figures/Tema 3/GradientCause.png}
\end{figure}

Sucede principalmente con las funciones \alert{tanh} y \alert{sigmoid}, por lo tanto se \alert{recomienda} el uso de ReLU para capas ocultas en una red.
\end{frame}

\begin{frame}{Notebook de ejemplo, perceptrón clasificador}
El siguiente notebook contiene un ejemplo de clasificador usando un perceptrón multicapa como red neuronal.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Slides/figures/GoogleColab.png}
\end{figure}
\begin{itemize}
    \centering
    \item {\Large \href{https://colab.research.google.com/github/guillermoih/M-todos-Generativos/blob/main/Notebooks/1.1_01-PerceptronClasificador.ipynb}{1.1\_1-PerceptronClasificador.ipynb}}
\end{itemize}
\end{frame}

\section{Perceptrón multicapa para procesar imágenes}

\begin{frame}{¿Cómo procesar imágenes?}
La idea más \alert{básica} para procesar imágenes con redes neuronales es  transformar la \alert{matriz numérica} de datos a un \alert{vector unidimensional}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Slides/figures/Tema 3/NNVideo.jpg}
    \caption{\href{https://www.youtube.com/watch?v=aircAruvnKk&t=218s}{Vídeo youtube}}
\end{figure}
\end{frame}

\begin{frame}{Capa de reshape}
La capa de \alert{keras} llamada \say{\alert{reshape}} se encarga de realizar transformaciones en los tensores. Ya que solo queremos convertir la imagen en un vector \say{plano}, también podemos utilizar la capa \say{\alert{flatten}} de \alert{matriz} a \alert{vector}.

Sin embargo el principal \alert{inconveniente} al tratar las imágenes de esta manera es la \alert{pérdida total} de información espacial de la imagen.
\end{frame}


\begin{frame}{Notebook de ejemplo, perceptrón clasificador de imágenes}
El siguiente notebook contiene un código parcial de clasificador de imágenes usando un perceptrón multicapa como red neuronal.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Slides/figures/GoogleColab.png}
\end{figure}
\begin{itemize}
    \centering
    \item {\Large \href{https://colab.research.google.com/github/guillermoih/M-todos-Generativos/blob/main/Notebooks/1.1_02-PerceptronImagenes.ipynb}{1.1\_2-PerceptronImagenes.ipynb}}
\end{itemize}
Si quieres usar estas redes sencillas en otros datasets puede probar con \href{https://archive.ics.uci.edu/dataset/109/wine}{Wine dataset} y con \href{https://github.com/zalandoresearch/fashion-mnist}{Fashion MNIST}.
\end{frame}
\addcontentsline{toc}{section}{Referencias}

\begin{frame}[allowframebreaks]{Referencias}
    \bibliographystyle{unsrt}
    \bibliography{Slides/references.bib}
\end{frame}

\begin{frame}{Contribuciones de las diapositivas}
\begin{itemize}
    \item \textbf{Autor original de las diapositivas:} Guillermo Iglesias Hernández
    \item \textbf{Extensión de contenido:} Jorge Dueñas Lerín
\end{itemize}
\end{frame}

\end{document}